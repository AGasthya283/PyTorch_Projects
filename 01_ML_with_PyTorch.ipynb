{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88d07f2d",
   "metadata": {},
   "source": [
    "# **Machine Learning with PyTorch**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac03cca3",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch\n",
    "\n",
    "PyTorch is an open-source machine learning library developed by Facebook's AI Research lab. It's primarily used for building and training neural networks, but it also supports other types of machine learning models.\n",
    "\n",
    "PyTorch's design philosophy is to be efficient, flexible, and easy to use. It's based on the Torch library, which has been used for many years in the research community, but PyTorch has added its own improvements and features.\n",
    "\n",
    "Here are some key features of PyTorch:\n",
    "\n",
    "- **Dynamic computation graph**: PyTorch allows you to define a computation graph dynamically during runtime, which makes it easier to debug and understand your models.\n",
    "\n",
    "- **Automatic differentiation**: PyTorch automatically computes gradients for you, which is essential for training neural networks.\n",
    "\n",
    "- **GPU acceleration**: PyTorch can run on GPUs, which can significantly speed up your computations.\n",
    "\n",
    "- **Large ecosystem**: PyTorch has a large and active community, which means there are many pre-built models and tools available.\n",
    "\n",
    "- **Pythonic syntax**: PyTorch uses Python syntax, which makes it easy to read and write code.\n",
    "\n",
    "Overall, PyTorch is a powerful and flexible library that's well-suited for a wide range of machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09291efe",
   "metadata": {},
   "source": [
    "## Step-1: Importing the Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed5ceece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b10361",
   "metadata": {},
   "source": [
    "### MNIST Dataset\n",
    "\n",
    "Here, we'll be working with the MNIST dataset, which contains over 60,000 images of handwritten digits. The dataset will be split into training and testing sets.\n",
    "<!--\n",
    "Here are the main steps we'll be following:\n",
    "\n",
    "1. **Download the MNIST dataset**: We'll download the MNIST dataset and load it into our environment.\n",
    "\n",
    "2. **Create a DataLoader for the dataset**: We'll create a DataLoader object to handle loading and preprocessing the data for our model.\n",
    "\n",
    "3. **Define an AI model to recognize a hand-written digit**: We'll define a neural network model that can recognize hand-written digits.\n",
    "\n",
    "4. **Train the defined AI model using training data from the MNIST dataset**: We'll train our model using the training data from the MNIST dataset.\n",
    "\n",
    "5. **Test the trained AI model using testing data from the MNIST dataset**: We'll test our trained model using the testing data from the MNIST dataset.\n",
    "\n",
    "6. **Evaluate the model**: We'll evaluate the performance of our model using various metrics.\n",
    "\n",
    "Overall, the goal of this lab is to build and train a neural network model that can recognize hand-written digits using the MNIST dataset.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b2819c",
   "metadata": {},
   "source": [
    "## Step-2: Downloading Datasets and Building DataLoader\n",
    "\n",
    "In this code cell, we're downloading and loading the MNIST dataset using the datasets module from PyTorch. The MNIST dataset is a popular dataset that contains images of handwritten digits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b68dc12",
   "metadata": {},
   "source": [
    "- Using the `MNIST` class from the `datasets` module download the training and testing data. The `root` parameter specifies the directory where the data will be saved. The `train` parameter specifies whether we want the training or testing data. The `download` parameter specifies whether we want to download the data if it's not already downloaded. The `transform` parameter specifies the transformation to apply to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a0a88ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from MNIST datasets.\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21898b1a",
   "metadata": {},
   "source": [
    "- Creating `DataLoader` objects to iterate over the data. The `DataLoader` class takes in a dataset and a batch size, and returns an iterator that can be used to load batches of data. We're passing in the `training_data` and `test_data` datasets that we downloaded earlier, and setting the batch size to 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0947bd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loaders to iterate over data\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d1eb3b",
   "metadata": {},
   "source": [
    "- Verify size of the training and testing data by multiplying the length of the `DataLoader` object by the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db873b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 60032\n",
      "Test data size: 10048\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data size:\", len(train_dataloader) * batch_size)\n",
    "print(\"Test data size:\", len(test_dataloader) * batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9628f8a",
   "metadata": {},
   "source": [
    "- Iterating over the `test_dataloader` object and printing the shape of the input data `X` and the shape and data type of the target data `y`. The X data is in the shape of `[N, C, H, W]`, where `N` is the number of samples, `C` is the number of channels (in this case, 1), `H` is the height of the image, and `W` is the width of the image. The `y` data is in the shape of `[N]`, where `N` is the number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ded6f8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ad8c06",
   "metadata": {},
   "source": [
    "## Step-3: Defining the Model\n",
    "\n",
    "In this code cell, we're defining a neural network model using the `NeuralNetwork` class that we defined earlier. The model takes in an image tensor and returns a tensor of logits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbab0b3",
   "metadata": {},
   "source": [
    "- Determine the device for training. We're checking if a GPU is available using `torch.cuda.is_available()`, and if not, we're checking if an Apple Silicon GPU is available using `torch.backends.mps.is_available()`. If neither is available, we're using the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c8efe5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "NVIDIA GeForce GTX 1050 Ti\n"
     ]
    }
   ],
   "source": [
    "# Get device for training.\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() # Apple Silicon GPU\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Printing Device Architecture\n",
    "if device.type == \"cuda\":\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "elif device.type == \"mps\":\n",
    "    print(torch.backends.mps.current_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18a6c70",
   "metadata": {},
   "source": [
    "- Defining the `NeuralNetwork` class that inherits from `nn.Module`. The `__init__` method initializes the model with an input size, a hidden size, and a number of classes. The `flatten` layer flattens the input image tensor into a 1D tensor. The `linear_relu_stack` layer is a sequential container that contains three linear layers, each followed by a ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac4f9b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, image_tensor):\n",
    "        image_tensor = self.flatten(image_tensor)\n",
    "        logits = self.linear_relu_stack(image_tensor)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6dab16",
   "metadata": {},
   "source": [
    "- Setting the input size, hidden size, and number of classes for the model, We're creating an instance of the `NeuralNetwork` class with these parameters, and moving it to the device we defined earlier using the `to()` method. We're then printing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a790b6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 28*28\n",
    "hidden_size = 512\n",
    "num_classes = 10\n",
    "\n",
    "model = NeuralNetwork(input_size, hidden_size, num_classes).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ab6a03",
   "metadata": {},
   "source": [
    "## Step-4: Training loop\n",
    "Here, we're defining the training loop for our model. The training loop takes in a dataloader, a model, a loss function, and an optimizer, and trains the model on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d36b07",
   "metadata": {},
   "source": [
    "- Setting the learning rate, loss function, and optimizer for our model, The `learning_rate` is set to 0.001. The loss function is set to `nn.CrossEntropyLoss()`, which is a common loss function for classification tasks. The optimizer is set to `torch.optim.Adam`, which is a common optimizer for training neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9591e84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3 # 0.001\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f9731c",
   "metadata": {},
   "source": [
    "- Defining the `train` function that takes in a dataloader, a model, a loss function, and an optimizer. The function sets the model to training mode, iterates over the dataloader, and performs a forward pass to compute the prediction, a backward pass to compute the gradient, and an update to the model parameters using the optimizer. It also prints the loss and the progress of the training loop every 100 batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6aaf892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "\n",
    "    for batch_num, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Forward pass to compute prediction\n",
    "        pred = model(X)\n",
    "        # Compute prediction error using loss function\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad() # zero any previous gradient calculations\n",
    "        loss.backward() # calculate gradient\n",
    "        optimizer.step() # update model parameters\n",
    "        \n",
    "        if batch_num > 0 and batch_num % 100 == 0:\n",
    "            loss, current = loss.item(), batch_num * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2261246f",
   "metadata": {},
   "source": [
    "## Step-5: Test Loop\n",
    "Here, we're defining the test loop for our model. The test loop takes in a dataloader, a model, and a loss function, and evaluates the model on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc28b80c",
   "metadata": {},
   "source": [
    "- Defining the `test` function that takes in a dataloader, a model, and a loss function. The function sets the model to evaluation mode, iterates over the dataloader, and computes the loss and accuracy of the model on the test data. It also prints the accuracy and average loss of the model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8436520a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    for X, y in dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        test_loss += loss_fn(pred, y).item()\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfe28a1",
   "metadata": {},
   "source": [
    "## Step-6: Model Training\n",
    "Here, we're training the model using the training loop that we defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8502ac43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/10\n",
      "loss: 0.279117  [ 6400/60000]\n",
      "loss: 0.191582  [12800/60000]\n",
      "loss: 0.258322  [19200/60000]\n",
      "loss: 0.153658  [25600/60000]\n",
      "loss: 0.322177  [32000/60000]\n",
      "loss: 0.127257  [38400/60000]\n",
      "loss: 0.243816  [44800/60000]\n",
      "loss: 0.277377  [51200/60000]\n",
      "loss: 0.163641  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.1%, Avg loss: 0.151130 \n",
      "\n",
      "Starting epoch 2/10\n",
      "loss: 0.089042  [ 6400/60000]\n",
      "loss: 0.094192  [12800/60000]\n",
      "loss: 0.113072  [19200/60000]\n",
      "loss: 0.025151  [25600/60000]\n",
      "loss: 0.150454  [32000/60000]\n",
      "loss: 0.049042  [38400/60000]\n",
      "loss: 0.107422  [44800/60000]\n",
      "loss: 0.160705  [51200/60000]\n",
      "loss: 0.101604  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.4%, Avg loss: 0.111046 \n",
      "\n",
      "Starting epoch 3/10\n",
      "loss: 0.075426  [ 6400/60000]\n",
      "loss: 0.039534  [12800/60000]\n",
      "loss: 0.139657  [19200/60000]\n",
      "loss: 0.017066  [25600/60000]\n",
      "loss: 0.072302  [32000/60000]\n",
      "loss: 0.044181  [38400/60000]\n",
      "loss: 0.061914  [44800/60000]\n",
      "loss: 0.139639  [51200/60000]\n",
      "loss: 0.087648  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.7%, Avg loss: 0.119952 \n",
      "\n",
      "Starting epoch 4/10\n",
      "loss: 0.022951  [ 6400/60000]\n",
      "loss: 0.014271  [12800/60000]\n",
      "loss: 0.053746  [19200/60000]\n",
      "loss: 0.020689  [25600/60000]\n",
      "loss: 0.045853  [32000/60000]\n",
      "loss: 0.130936  [38400/60000]\n",
      "loss: 0.056436  [44800/60000]\n",
      "loss: 0.097402  [51200/60000]\n",
      "loss: 0.047502  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.1%, Avg loss: 0.149596 \n",
      "\n",
      "Starting epoch 5/10\n",
      "loss: 0.005843  [ 6400/60000]\n",
      "loss: 0.022868  [12800/60000]\n",
      "loss: 0.054935  [19200/60000]\n",
      "loss: 0.026449  [25600/60000]\n",
      "loss: 0.082391  [32000/60000]\n",
      "loss: 0.033509  [38400/60000]\n",
      "loss: 0.022711  [44800/60000]\n",
      "loss: 0.115966  [51200/60000]\n",
      "loss: 0.064509  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.5%, Avg loss: 0.093360 \n",
      "\n",
      "Starting epoch 6/10\n",
      "loss: 0.029625  [ 6400/60000]\n",
      "loss: 0.070339  [12800/60000]\n",
      "loss: 0.003923  [19200/60000]\n",
      "loss: 0.057219  [25600/60000]\n",
      "loss: 0.015930  [32000/60000]\n",
      "loss: 0.003573  [38400/60000]\n",
      "loss: 0.004100  [44800/60000]\n",
      "loss: 0.028364  [51200/60000]\n",
      "loss: 0.018768  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.7%, Avg loss: 0.096268 \n",
      "\n",
      "Starting epoch 7/10\n",
      "loss: 0.053424  [ 6400/60000]\n",
      "loss: 0.031373  [12800/60000]\n",
      "loss: 0.107403  [19200/60000]\n",
      "loss: 0.011795  [25600/60000]\n",
      "loss: 0.059702  [32000/60000]\n",
      "loss: 0.040038  [38400/60000]\n",
      "loss: 0.003581  [44800/60000]\n",
      "loss: 0.035029  [51200/60000]\n",
      "loss: 0.008535  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.092741 \n",
      "\n",
      "Starting epoch 8/10\n",
      "loss: 0.009851  [ 6400/60000]\n",
      "loss: 0.026976  [12800/60000]\n",
      "loss: 0.006265  [19200/60000]\n",
      "loss: 0.147219  [25600/60000]\n",
      "loss: 0.017752  [32000/60000]\n",
      "loss: 0.002964  [38400/60000]\n",
      "loss: 0.068261  [44800/60000]\n",
      "loss: 0.067436  [51200/60000]\n",
      "loss: 0.000737  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.098508 \n",
      "\n",
      "Starting epoch 9/10\n",
      "loss: 0.002913  [ 6400/60000]\n",
      "loss: 0.021794  [12800/60000]\n",
      "loss: 0.001137  [19200/60000]\n",
      "loss: 0.002753  [25600/60000]\n",
      "loss: 0.002323  [32000/60000]\n",
      "loss: 0.003603  [38400/60000]\n",
      "loss: 0.002449  [44800/60000]\n",
      "loss: 0.025128  [51200/60000]\n",
      "loss: 0.005422  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.084012 \n",
      "\n",
      "Starting epoch 10/10\n",
      "loss: 0.001460  [ 6400/60000]\n",
      "loss: 0.007849  [12800/60000]\n",
      "loss: 0.071389  [19200/60000]\n",
      "loss: 0.001145  [25600/60000]\n",
      "loss: 0.001535  [32000/60000]\n",
      "loss: 0.001839  [38400/60000]\n",
      "loss: 0.003327  [44800/60000]\n",
      "loss: 0.039231  [51200/60000]\n",
      "loss: 0.024599  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.082652 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Starting epoch {epoch+1}/{epochs}\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b947dc49",
   "metadata": {},
   "source": [
    "## Step-7: Saving the Model parameters to make predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdb6318",
   "metadata": {},
   "source": [
    "- Saving the model parameters to a file named `ml_with_pytorch_model.pth` using the `torch.save()` function. We're passing in the model state dictionary as the first argument and the file name as the second argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7f9b4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to ml_with_pytorch_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Save our model parameters\n",
    "torch.save(model.state_dict(), \"ml_with_pytorch_model.pth\")\n",
    "print(\"Saved PyTorch Model State to ml_with_pytorch_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b23db71",
   "metadata": {},
   "source": [
    "- Loading the saved model parameters into a new instance of the model using the `model.load_state_dict()` function. We're passing in the saved model parameters as the first argument and the model instance as the second argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f29dfceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved model parameters into a new instance of the model\n",
    "model = NeuralNetwork(input_size, hidden_size, num_classes).to(device)\n",
    "model.load_state_dict(torch.load(\"ml_with_pytorch_model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b29c91",
   "metadata": {},
   "source": [
    "- Set the model to **evaluation mode** with `model.eval()`.  \n",
    "- Loop through the **first 10 samples** in the test dataset and generate predictions using the model.  \n",
    "- Pass each test sample to the model to obtain its **prediction tensor**.  \n",
    "- Determine the **predicted class label** by applying `argmax(0).item()` to the prediction tensor.  \n",
    "- Display both the **predicted** and **actual** class labels for each test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1c19e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"7\", Actual: \"7\"\n",
      "Predicted: \"2\", Actual: \"2\"\n",
      "Predicted: \"1\", Actual: \"1\"\n",
      "Predicted: \"0\", Actual: \"0\"\n",
      "Predicted: \"4\", Actual: \"4\"\n",
      "Predicted: \"1\", Actual: \"1\"\n",
      "Predicted: \"4\", Actual: \"4\"\n",
      "Predicted: \"9\", Actual: \"9\"\n",
      "Predicted: \"5\", Actual: \"5\"\n",
      "Predicted: \"9\", Actual: \"9\"\n"
     ]
    }
   ],
   "source": [
    "# Inference using the new model instance\n",
    "model.eval()\n",
    "for i in range(10):\n",
    "    x, y = test_data[i][0], test_data[i][1]\n",
    "\n",
    "    x = x.to(device)\n",
    "    pred = model(x)\n",
    "    predicted, actual = pred[0].argmax(0).item(), y\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
